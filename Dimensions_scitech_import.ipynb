{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "##  Made by: Dr. Keungoui Kim\n",
    "##  Title: Scitech data import from Dimensions\n",
    "##  goal : \n",
    "##  Data set:  \n",
    "##  Time Span:\n",
    "##  Variables\n",
    "##      Input: \n",
    "##      Output: \n",
    "##  Time-stamp: #  \"Sun Jan 26 01:47:34 2020\":  edited by awe kim ; code\n",
    "##  Notice :\n",
    "#######################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "### Directory Setting for Export\n",
    "#dir = \"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/\" # Home\n",
    "dir = \"D:/Google Drive(awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/\" # HGU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mDimcli - Dimensions API Client (v0.9.9.1)\u001b[0m\n",
      "\u001b[2mConnected to: <https://app.dimensions.ai/api/dsl/v2> - DSL v2.5\u001b[0m\n",
      "\u001b[2mMethod: manual login\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "### API log in\n",
    "import dimcli\n",
    "dimcli.login(key=\"792DDFAFCCA7478D8F37159F274A2783\",\n",
    "             endpoint=\"https://app.dimensions.ai/api/dsl/v2\")\n",
    "\n",
    "dsl = dimcli.Dsl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting iteration with limit=1000 skip=0 ...\n",
      "0-1000 / 12332 (10.04s)\n",
      "1000-2000 / 12332 (5.32s)\n",
      "2000-3000 / 12332 (4.10s)\n",
      "3000-4000 / 12332 (4.69s)\n",
      "4000-5000 / 12332 (4.85s)\n",
      "5000-6000 / 12332 (6.19s)\n",
      "6000-7000 / 12332 (4.84s)\n",
      "7000-8000 / 12332 (4.53s)\n",
      "8000-9000 / 12332 (4.95s)\n",
      "9000-10000 / 12332 (5.99s)\n",
      "10000-11000 / 12332 (8.05s)\n",
      "11000-12000 / 12332 (3.11s)\n",
      "12000-12332 / 12332 (2.19s)\n",
      "===\n",
      "Records extracted: 12332\n"
     ]
    }
   ],
   "source": [
    "### Extract patent data\n",
    "pat_data = dsl.query_iterative(\"\"\"search patents\n",
    "                                  where publications is not empty and year = 2022                    \n",
    "                                  return patents[id+year+priority_year+cpc+publications+publication_ids]\"\"\") #  limit 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12332, 6)\n",
      "12332\n"
     ]
    }
   ],
   "source": [
    "pat_data_df = pat_data.as_dataframe()\n",
    "print(pat_data_df.shape)\n",
    "print(pat_data_df.id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cpc</th>\n",
       "      <th>priority_year</th>\n",
       "      <th>publication_ids</th>\n",
       "      <th>publications</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WO-2022236335-A1</td>\n",
       "      <td>[A61K38/177, A61P29/00, C07K14/70578, A61P37/0...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[pub.1027342753, pub.1009940936]</td>\n",
       "      <td>[{'doi': '10.1182/blood-2003-06-2031', 'id': '...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WO-2022236333-A2</td>\n",
       "      <td>[A61B3/0016, A61B3/0033]</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[pub.1065228315]</td>\n",
       "      <td>[{'doi': '10.1364/ol.35.000739', 'id': 'pub.10...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO-2022236331-A1</td>\n",
       "      <td>[B05B12/18, B05B12/16, B05B14/00, B05B7/0408]</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[pub.1103595211]</td>\n",
       "      <td>[{'doi': '10.1002/adem.201701084', 'id': 'pub....</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WO-2022236308-A1</td>\n",
       "      <td>[A61B5/0042, A61B34/30, G01R33/3806, G01R33/34...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[pub.1092392383]</td>\n",
       "      <td>[{'doi': '10.1109/tmag.2017.2751001', 'id': 'p...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WO-2022236297-A1</td>\n",
       "      <td>[C11D3/48, C11D3/0068, C11D3/381, C11D7/40, C1...</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>[pub.1080312358]</td>\n",
       "      <td>[{'doi': '10.1128/jb.96.2.479-486.1968', 'id':...</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                                cpc  \\\n",
       "0  WO-2022236335-A1  [A61K38/177, A61P29/00, C07K14/70578, A61P37/0...   \n",
       "1  WO-2022236333-A2                           [A61B3/0016, A61B3/0033]   \n",
       "2  WO-2022236331-A1      [B05B12/18, B05B12/16, B05B14/00, B05B7/0408]   \n",
       "3  WO-2022236308-A1  [A61B5/0042, A61B34/30, G01R33/3806, G01R33/34...   \n",
       "4  WO-2022236297-A1  [C11D3/48, C11D3/0068, C11D3/381, C11D7/40, C1...   \n",
       "\n",
       "   priority_year                   publication_ids  \\\n",
       "0         2021.0  [pub.1027342753, pub.1009940936]   \n",
       "1         2021.0                  [pub.1065228315]   \n",
       "2         2021.0                  [pub.1103595211]   \n",
       "3         2021.0                  [pub.1092392383]   \n",
       "4         2021.0                  [pub.1080312358]   \n",
       "\n",
       "                                        publications  year  \n",
       "0  [{'doi': '10.1182/blood-2003-06-2031', 'id': '...  2022  \n",
       "1  [{'doi': '10.1364/ol.35.000739', 'id': 'pub.10...  2022  \n",
       "2  [{'doi': '10.1002/adem.201701084', 'id': 'pub....  2022  \n",
       "3  [{'doi': '10.1109/tmag.2017.2751001', 'id': 'p...  2022  \n",
       "4  [{'doi': '10.1128/jb.96.2.479-486.1968', 'id':...  2022  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_data_df.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pat_data_df_\"+\"2022\"+\".csv\", index=False)\n",
    "pat_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import .csv files\n",
    "# pat_data_df = pd.read_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pat_data_df_01.csv\")\n",
    "\n",
    "# from ast import literal_eval\n",
    "\n",
    "# pat_data_df['publication_ids'] = pat_data_df['publication_ids'].apply(literal_eval) #convert to list type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>publication_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WO-2022236335-A1</td>\n",
       "      <td>pub.1027342753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WO-2022236335-A1</td>\n",
       "      <td>pub.1009940936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WO-2022236333-A2</td>\n",
       "      <td>pub.1065228315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WO-2022236331-A1</td>\n",
       "      <td>pub.1103595211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WO-2022236308-A1</td>\n",
       "      <td>pub.1092392383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id publication_ids\n",
       "0  WO-2022236335-A1  pub.1027342753\n",
       "1  WO-2022236335-A1  pub.1009940936\n",
       "2  WO-2022236333-A2  pub.1065228315\n",
       "3  WO-2022236331-A1  pub.1103595211\n",
       "4  WO-2022236308-A1  pub.1092392383"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat_pub_df = pat_data_df[['id','publication_ids']]\n",
    "pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "pat_pub_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Extract publication_ids\n",
    "pub_list = pat_pub_df.publication_ids.unique()\n",
    "pd.DataFrame(pub_list, columns=[\"publication_ids\"]).to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pub_list_df_\"+\"2022\"+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finalized Version\n",
    "- Maximum 50,000 records\n",
    "- dimcli query cannot recognize ''... so I had to write the loop for each jurisdiction --> fixed\n",
    "- If I could convert dataframe's column into a big list with \"\" as elements, I would use Version 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Codes for extracting patent data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create patent data.frame\n",
    "years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'] #'2000','2001','2002','2003','2004','2005','2006','2007','2008','2009'\n",
    "offices = ['EP'] # 'WO', 'US', 'EP'\n",
    "\n",
    "for year in years:\n",
    "    for office in offices:\n",
    "        ### Patent set\n",
    "        query = \"\"\"search patents \n",
    "                    where publications is not empty and jurisdiction = \"\"\"+ '''\"'''+office+'''\"''' +\"\"\" and priority_year=\"\"\" + year + \"\"\" \n",
    "                    return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "        pat_data = dsl.query_iterative(query) \n",
    "        pat_df = pat_data.as_dataframe()\n",
    "\n",
    "        # Export patent set\n",
    "        pat_df.to_csv(dir+\"pat_data_df_\"+year+\"_\"+office+\".csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative Codes for extracting publication data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n",
      "0-1000 / 6576 (1.90s)\u001b[0m\n",
      "1000-2000 / 6576 (1.84s)\u001b[0m\n",
      "2000-3000 / 6576 (1.84s)\u001b[0m\n",
      "3000-4000 / 6576 (4.44s)\u001b[0m\n",
      "4000-5000 / 6576 (2.00s)\u001b[0m\n",
      "5000-6000 / 6576 (1.90s)\u001b[0m\n",
      "6000-6576 / 6576 (6.43s)\u001b[0m\n",
      "===\n",
      "Records extracted: 6576\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 7872 (3.92s)\u001b[0m\n",
      "1000-2000 / 7872 (5.93s)\u001b[0m\n",
      "2000-3000 / 7872 (1.90s)\u001b[0m\n",
      "3000-4000 / 7872 (2.76s)\u001b[0m\n",
      "4000-5000 / 7872 (6.05s)\u001b[0m\n",
      "5000-6000 / 7872 (5.86s)\u001b[0m\n",
      "6000-7000 / 7872 (6.40s)\u001b[0m\n",
      "7000-7872 / 7872 (5.51s)\u001b[0m\n",
      "===\n",
      "Records extracted: 7872\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 8547 (2.16s)\u001b[0m\n",
      "1000-2000 / 8547 (1.84s)\u001b[0m\n",
      "2000-3000 / 8547 (1.94s)\u001b[0m\n",
      "3000-4000 / 8547 (1.93s)\u001b[0m\n",
      "4000-5000 / 8547 (1.88s)\u001b[0m\n",
      "5000-6000 / 8547 (1.77s)\u001b[0m\n",
      "6000-7000 / 8547 (1.91s)\u001b[0m\n",
      "7000-8000 / 8547 (1.89s)\u001b[0m\n",
      "8000-8547 / 8547 (4.24s)\u001b[0m\n",
      "===\n",
      "Records extracted: 8547\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 8934 (1.86s)\u001b[0m\n",
      "1000-2000 / 8934 (2.59s)\u001b[0m\n",
      "2000-3000 / 8934 (6.16s)\u001b[0m\n",
      "3000-4000 / 8934 (1.78s)\u001b[0m\n",
      "4000-5000 / 8934 (4.63s)\u001b[0m\n",
      "5000-6000 / 8934 (2.61s)\u001b[0m\n",
      "6000-7000 / 8934 (5.20s)\u001b[0m\n",
      "7000-8000 / 8934 (1.77s)\u001b[0m\n",
      "8000-8934 / 8934 (2.28s)\u001b[0m\n",
      "===\n",
      "Records extracted: 8934\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2003 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 10150 (4.94s)\u001b[0m\n",
      "1000-2000 / 10150 (1.75s)\u001b[0m\n",
      "2000-3000 / 10150 (2.87s)\u001b[0m\n",
      "3000-4000 / 10150 (1.95s)\u001b[0m\n",
      "4000-5000 / 10150 (4.49s)\u001b[0m\n",
      "5000-6000 / 10150 (6.86s)\u001b[0m\n",
      "6000-7000 / 10150 (5.21s)\u001b[0m\n",
      "7000-8000 / 10150 (1.93s)\u001b[0m\n",
      "8000-9000 / 10150 (4.39s)\u001b[0m\n",
      "9000-10000 / 10150 (1.92s)\u001b[0m\n",
      "10000-10150 / 10150 (2.13s)\u001b[0m\n",
      "===\n",
      "Records extracted: 10150\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 12615 (6.36s)\u001b[0m\n",
      "1000-2000 / 12615 (2.01s)\u001b[0m\n",
      "2000-3000 / 12615 (4.14s)\u001b[0m\n",
      "3000-4000 / 12615 (6.00s)\u001b[0m\n",
      "4000-5000 / 12615 (5.86s)\u001b[0m\n",
      "5000-6000 / 12615 (1.78s)\u001b[0m\n",
      "6000-7000 / 12615 (1.86s)\u001b[0m\n",
      "7000-8000 / 12615 (1.82s)\u001b[0m\n",
      "8000-9000 / 12615 (7.09s)\u001b[0m\n",
      "9000-10000 / 12615 (1.93s)\u001b[0m\n",
      "10000-11000 / 12615 (1.87s)\u001b[0m\n",
      "11000-12000 / 12615 (1.86s)\u001b[0m\n",
      "12000-12615 / 12615 (1.63s)\u001b[0m\n",
      "===\n",
      "Records extracted: 12615\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2005 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 14231 (4.56s)\u001b[0m\n",
      "1000-2000 / 14231 (1.89s)\u001b[0m\n",
      "2000-3000 / 14231 (1.74s)\u001b[0m\n",
      "3000-4000 / 14231 (1.88s)\u001b[0m\n",
      "4000-5000 / 14231 (1.75s)\u001b[0m\n",
      "5000-6000 / 14231 (1.84s)\u001b[0m\n",
      "6000-7000 / 14231 (1.97s)\u001b[0m\n",
      "7000-8000 / 14231 (1.78s)\u001b[0m\n",
      "8000-9000 / 14231 (4.70s)\u001b[0m\n",
      "9000-10000 / 14231 (5.80s)\u001b[0m\n",
      "10000-11000 / 14231 (6.22s)\u001b[0m\n",
      "11000-12000 / 14231 (1.81s)\u001b[0m\n",
      "12000-13000 / 14231 (2.70s)\u001b[0m\n",
      "13000-14000 / 14231 (1.89s)\u001b[0m\n",
      "14000-14231 / 14231 (1.97s)\u001b[0m\n",
      "===\n",
      "Records extracted: 14231\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 16209 (4.83s)\u001b[0m\n",
      "1000-2000 / 16209 (1.78s)\u001b[0m\n",
      "2000-3000 / 16209 (2.77s)\u001b[0m\n",
      "3000-4000 / 16209 (5.85s)\u001b[0m\n",
      "4000-5000 / 16209 (3.98s)\u001b[0m\n",
      "5000-6000 / 16209 (2.25s)\u001b[0m\n",
      "6000-7000 / 16209 (1.92s)\u001b[0m\n",
      "7000-8000 / 16209 (4.47s)\u001b[0m\n",
      "8000-9000 / 16209 (6.07s)\u001b[0m\n",
      "9000-10000 / 16209 (1.77s)\u001b[0m\n",
      "10000-11000 / 16209 (4.52s)\u001b[0m\n",
      "11000-12000 / 16209 (1.90s)\u001b[0m\n",
      "12000-13000 / 16209 (4.41s)\u001b[0m\n",
      "13000-14000 / 16209 (6.02s)\u001b[0m\n",
      "14000-15000 / 16209 (6.00s)\u001b[0m\n",
      "15000-16000 / 16209 (3.04s)\u001b[0m\n",
      "16000-16209 / 16209 (1.35s)\u001b[0m\n",
      "===\n",
      "Records extracted: 16209\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 16977 (4.74s)\u001b[0m\n",
      "1000-2000 / 16977 (1.82s)\u001b[0m\n",
      "2000-3000 / 16977 (4.49s)\u001b[0m\n",
      "3000-4000 / 16977 (1.74s)\u001b[0m\n",
      "4000-5000 / 16977 (2.94s)\u001b[0m\n",
      "5000-6000 / 16977 (1.93s)\u001b[0m\n",
      "6000-7000 / 16977 (4.48s)\u001b[0m\n",
      "7000-8000 / 16977 (6.30s)\u001b[0m\n",
      "8000-9000 / 16977 (5.66s)\u001b[0m\n",
      "9000-10000 / 16977 (1.87s)\u001b[0m\n",
      "10000-11000 / 16977 (2.81s)\u001b[0m\n",
      "11000-12000 / 16977 (1.96s)\u001b[0m\n",
      "12000-13000 / 16977 (1.86s)\u001b[0m\n",
      "13000-14000 / 16977 (1.91s)\u001b[0m\n",
      "14000-15000 / 16977 (4.56s)\u001b[0m\n",
      "15000-16000 / 16977 (1.98s)\u001b[0m\n",
      "16000-16977 / 16977 (2.34s)\u001b[0m\n",
      "===\n",
      "Records extracted: 16977\u001b[0m\n",
      "Starting iteration with limit=1000 skip=0 ...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 EP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0-1000 / 17902 (5.71s)\u001b[0m\n",
      "1000-2000 / 17902 (1.72s)\u001b[0m\n",
      "2000-3000 / 17902 (1.85s)\u001b[0m\n",
      "3000-4000 / 17902 (1.71s)\u001b[0m\n",
      "4000-5000 / 17902 (1.88s)\u001b[0m\n",
      "5000-6000 / 17902 (4.54s)\u001b[0m\n",
      "6000-7000 / 17902 (7.50s)\u001b[0m\n",
      "7000-8000 / 17902 (4.47s)\u001b[0m\n",
      "8000-9000 / 17902 (6.16s)\u001b[0m\n",
      "9000-10000 / 17902 (1.99s)\u001b[0m\n",
      "10000-11000 / 17902 (2.44s)\u001b[0m\n",
      "11000-12000 / 17902 (2.00s)\u001b[0m\n",
      "12000-13000 / 17902 (3.64s)\u001b[0m\n",
      "13000-14000 / 17902 (1.83s)\u001b[0m\n",
      "14000-15000 / 17902 (4.49s)\u001b[0m\n",
      "15000-16000 / 17902 (6.08s)\u001b[0m\n",
      "16000-17000 / 17902 (1.88s)\u001b[0m\n",
      "17000-17902 / 17902 (1.87s)\u001b[0m\n",
      "===\n",
      "Records extracted: 17902\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009 EP\n"
     ]
    }
   ],
   "source": [
    "### Create patent data.frame\n",
    "# 230102 - 2006 \n",
    "years = ['2000','2001','2002','2003','2004','2005','2006','2007','2008','2009'] #'2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'\n",
    "offices = ['EP']\n",
    "\n",
    "for year in years:\n",
    "    for office in offices:\n",
    "        ### Patent set\n",
    "        query = \"\"\"search patents \n",
    "                    where publications is not empty and jurisdiction = \"\"\"+ '''\"'''+office+'''\"''' +\"\"\" and priority_year=\"\"\" + year + \"\"\" \n",
    "                    return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "        pat_data = dsl.query_iterative(query) \n",
    "        pat_df = pat_data.as_dataframe()\n",
    "\n",
    "        # Export patent set\n",
    "        pat_df.to_csv(dir+\"pat_data_df_\"+year+\"_\"+office+\".csv\", index=False)\n",
    "\n",
    "        # Create publication set\n",
    "        pat_pub_df = pat_df[['id','publication_ids']]\n",
    "        pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "        pat_pub_df.to_csv(dir+\"pat_pub_df_\"+year+\"_\"+office+\".csv\", index=False)\n",
    "\n",
    "        # Create CPC set\n",
    "        pat_cpc_df = pat_df[['id','cpc']]\n",
    "        pat_cpc_df = pat_cpc_df.explode(\"cpc\", ignore_index=True)\n",
    "        pat_cpc_df.to_csv(dir+\"pat_cpc_df_\"+year+\"_\"+office+\".csv\", index=False)\n",
    "\n",
    "        ## Export Publication List version 2. 50,000 each\n",
    "        pub_list = pat_pub_df.publication_ids.unique()\n",
    "        pub_list = pd.DataFrame(pub_list, columns=[\"publication_ids\"])\n",
    "        loop = math.ceil(pub_list.shape[0]/50000)+1\n",
    "        for i in range(1,loop):\n",
    "            temp = pub_list.iloc[0:49999,]\n",
    "            temp.to_csv(dir+\"pub_list_df_\"+year+\"_\"+office+str(i)+\".csv\", index=False)\n",
    "\n",
    "            # in operator only can contain maximum 512 elements\n",
    "            # Thus, I run another loop for extracting 500 publication ids\n",
    "            inloop = math.ceil(temp.shape[0]/500)+1\n",
    "            pub_df = pd.DataFrame() # Create empty data.frame  \n",
    "            for j in range(1, inloop):\n",
    "                intemp = temp.iloc[0:499,]\n",
    "                query = \"\"\"search publications \n",
    "                        where id in \"\"\"+'''['''+','.join(f'\"{x}\"' for x in intemp['publication_ids'].to_list())+''']'''+\"\"\"\n",
    "                        return publications[id+type+volume+year+issue+title+journal+authors+category_for]\"\"\"\n",
    "                intemp_data = dsl.query_iterative(query)\n",
    "                intemp_data = intemp_data.as_dataframe()    \n",
    "                \n",
    "                pub_df = pd.concat([pub_df, intemp_data], axis=0)\n",
    "                temp = temp.drop(temp.index[0:499])          \n",
    "\n",
    "\n",
    "            # Save publication set\n",
    "            pub_df.to_csv(dir+\"pub_data_df_\"+year+\"_\"+office+\"_\"+str(i)+\".csv\", index=False)\n",
    "            pub_list = pub_list.drop(pub_list.index[0:49999])          \n",
    "\n",
    "    print(year, office)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create publication list\n",
    "years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'] \n",
    "offices = ['EP']\n",
    "\n",
    "for year in years:\n",
    "    for office in offices:\n",
    "        # Create publication set\n",
    "        pat_pub_df = pat_df[['id','publication_ids']]\n",
    "        pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "\n",
    "        ## Export Publication List version 1. ALL\n",
    "        pub_list = pat_pub_df.publication_ids.unique()\n",
    "        pub_list = pd.DataFrame(pub_list, columns=[\"publication_ids\"])\n",
    "        pub_list.to_csv(dir+\"pub_list_df_\"+year+\"_\"+office+\".csv\", index=False)\n",
    "\n",
    "        ## Export Publication List version 2. 50,000 each\n",
    "        pub_list = pat_pub_df.publication_ids.unique()\n",
    "        pub_list = pd.DataFrame(pub_list, columns=[\"publication_ids\"])\n",
    "        loop = math.ceil(pub_list.shape[0]/50000)+1\n",
    "        for i in range(1,loop):\n",
    "            temp = pub_list.iloc[0:49999,]\n",
    "            temp.to_csv(dir+\"pub_list_df_\"+year+\"_\"+office+str(i)+\".csv\", index=False)\n",
    "            pub_list = pub_list.drop(pub_list.index[0:49999])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create publication data.frame\n",
    "years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'] \n",
    "offices = ['EP']\n",
    "\n",
    "for year in years:\n",
    "    for office in offices:\n",
    "        ### Publication set\n",
    "\n",
    "        ## Version 1. Request query for single publication id --> takes longer time\n",
    "        pub_list = pd.read_csv(dir+\"pub_list_df_\"+year+\"_\"+office+\".csv\")\n",
    "        # First record\n",
    "#        query = \"\"\"search publications \n",
    "#                    where id = \"\"\"+'''\"'''+pub_list.publication_ids[0]+'''\"'''+\"\"\"\n",
    "#                    return publications[id+type+volume+year+issue+title+journal+authors]\"\"\"\n",
    "#        temp_data = dsl.query(query)    \n",
    "#        pub_df = temp_data.as_dataframe()\n",
    "\n",
    "#        for pub in range(1,len(pub_list.publication_ids)):\n",
    "#            query = \"\"\"search publications \n",
    "#                        where id = \"\"\"+'''\"'''+pub_list.publication_ids[pub]+'''\"'''+\"\"\"\n",
    "#                        return publications[id+type+volume+year+issue+title+journal+authors+category_for]\"\"\"\n",
    "#            temp_data = dsl.query(query)    \n",
    "#            temp_data = temp_data.as_dataframe()\n",
    "#            \n",
    "#           # bind rows\n",
    "#           pub_df = pd.concat([pub_df, temp_data], axis=0)\n",
    "#\n",
    "#           print(year, office, pub)\n",
    "\n",
    "        # Version 2.\n",
    "        query = \"\"\"search publications \n",
    "                   where id in \"\"\"+'''['''+','.join(f'\"{x}\"' for x in pub_list['0'].to_list())+''']'''+\"\"\"\n",
    "                   return publications[id+type+volume+year+issue+title+journal+authors+category_for]\"\"\"\n",
    "        temp_data = dsl.query_iterative(query)\n",
    "        pub_df = temp_data.as_dataframe()    \n",
    "\n",
    "        # Save publication set\n",
    "        pub_df.to_csv(dir+\"pub_data_df_\"+year+\"_\"+office+\".csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Publication list manually\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(dir+\"/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Total Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check total counts\n",
    "\n",
    "years = ['2010','2011','2012','2013','2014','2015','2016','2017','2018','2019','2020','2021','2022'] \n",
    "totalRecUS = []\n",
    "totalRecEU = []\n",
    "totalRecWO = []\n",
    "for year in years:\n",
    "    ### US\n",
    "    query = \"\"\"search patents \n",
    "                    where publications is not empty and jurisdiction in [\"US\"] and priority_year=\"\"\" + year + \"\"\" \n",
    "                    return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "    pat_data = dsl.query(query) \n",
    "    totalRecUS.append(pat_data.count_total)\n",
    "\n",
    "    ### EP\n",
    "    query = \"\"\"search patents \n",
    "            where publications is not empty and jurisdiction in [\"EP\"] and priority_year=\"\"\" + year + \"\"\" \n",
    "            return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "    pat_data = dsl.query(query) \n",
    "    totalRecEUWO.append(pat_data.count_total)\n",
    "\n",
    "    ### WO\n",
    "    query = \"\"\"search patents \n",
    "            where publications is not empty and jurisdiction in [\"WO\"] and priority_year=\"\"\" + year + \"\"\" \n",
    "            return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "    pat_data = dsl.query(query) \n",
    "    totalRecEUWO.append(pat_data.count_total)\n",
    "\n",
    "    print(year)\n",
    "\n",
    "# Present result\n",
    "pd.DataFrame({'year':years,'USRec':totalRecUS,'EUWORec':totalRecEUWO})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = '2021'\n",
    "\n",
    "for year in years:\n",
    "    query = \"\"\"search patents \n",
    "            where publications is not empty and year=\"\"\" + year + \"\"\"\n",
    "            return patents[id+year+priority_year+cpc+publications+publication_ids]\"\"\"\n",
    "    pat_data = dsl.query_iterative(query) \n",
    "    pat_df = pat_data.as_dataframe()\n",
    "    rangeIndex = int(temp.count_total/1000)+1\n",
    "\n",
    "    for t in range(50,rangeIndex):\n",
    "        query = \"\"\"search patents \n",
    "            where publications is not empty and year=\"\"\" + year + \"\"\"\n",
    "            return patents[id+year+priority_year+cpc+publications+publication_ids+reference_ids+times_cited] limit 1000 skip \"\"\" + str(1000*t)\n",
    "        temp_data = dsl.query(query)\n",
    "        temp_df = temp_data.as_dataframe()\n",
    "        # bind rows\n",
    "        pat_df = pd.concat([pat_df, temp_df], axis=0)\n",
    "        print(t)\n",
    "\n",
    "    # Save patent set\n",
    "    pat_data_df.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pat_data_df_\"+year+\".csv\", index=False)\n",
    "\n",
    "    # Save publication set\n",
    "    pat_pub_df = pat_data_df[['id','publication_ids']]\n",
    "    pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "\n",
    "    # Extract publication_ids\n",
    "    pub_list = pat_pub_df.publication_ids.unique()\n",
    "    pd.DataFrame(pub_list, columns=[\"publication_ids\"]).to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pub_list_df_\"+year+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%dslloop is a cell magic, but the cell body is empty. Did you mean the line magic %dslloop (single %)?\n"
     ]
    }
   ],
   "source": [
    "# %%dslloop my_data search publications for \"malaria\" return publications limit 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EP #####\n",
    "    query = \"\"\"search patents \n",
    "                where publications is not empty and jurisdiction in [\"EP\"] and priority_year=\"\"\" + year + \"\"\" \n",
    "                return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "    pat_data = dsl.query_iterative(query) \n",
    "    pat_df = pat_data.as_dataframe()\n",
    "\n",
    "    # Save patent set\n",
    "    pat_df.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pat_data_df_\"+year+\"_EP.csv\", index=False)\n",
    "\n",
    "    # Save publication set\n",
    "    pat_pub_df = pat_df[['id','publication_ids']]\n",
    "    pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "\n",
    "    # Extract publication_ids\n",
    "    pub_list = pat_pub_df.publication_ids.unique()\n",
    "    pub_list = pd.DataFrame(pub_list, columns=[\"publication_ids\"])\n",
    "\n",
    "    loop = math.ceil(pub_list.shape[0]/50000)+1\n",
    "    \n",
    "    for i in range(1,loop):\n",
    "        temp = pub_list.iloc[0:49999,]\n",
    "        temp.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pub_list_df_\"+year+\"_EP_\"+str(i)+\".csv\")\n",
    "        pub_list = pub_list.drop(pub_list.index[0:49999])\n",
    "\n",
    "    ##### WO #####\n",
    "    query = \"\"\"search patents \n",
    "                where publications is not empty and jurisdiction in [\"WO\"] and priority_year=\"\"\" + year + \"\"\" \n",
    "                return patents[id+family_id+priority_year+priority_date+jurisdiction+cpc+publication_ids] sort by priority_date\"\"\"\n",
    "    pat_data = dsl.query_iterative(query) \n",
    "    pat_df = pat_data.as_dataframe()\n",
    "\n",
    "    # Save patent set\n",
    "    pat_df.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pat_data_df_\"+year+\"_WO.csv\", index=False)\n",
    "\n",
    "    # Save publication set\n",
    "    pat_pub_df = pat_df[['id','publication_ids']]\n",
    "    pat_pub_df = pat_pub_df.explode(\"publication_ids\", ignore_index=True)\n",
    "\n",
    "    # Extract publication_ids\n",
    "    pub_list = pat_pub_df.publication_ids.unique()\n",
    "    pub_list = pd.DataFrame(pub_list, columns=[\"publication_ids\"])\n",
    "\n",
    "    loop = math.ceil(pub_list.shape[0]/50000)+1\n",
    "    \n",
    "    for i in range(1,loop):\n",
    "        temp = pub_list.iloc[0:49999,]\n",
    "        temp.to_csv(\"E:/Google Drive (awekim@handong.edu)/[Research]/00_Dimensions/Dimension_files/pub_list_df_\"+year+\"_WO_\"+str(i)+\".csv\")\n",
    "        pub_list = pub_list.drop(pub_list.index[0:49999])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a077222d77dfe082b8f1dd562ad70e458ac2ab76993a0b248ab0476e32e9e8dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
